"""
Evaluate the filtering experiments using the following:
accuracy, precision, recall, f1-score, specificity, confusion matrix
As well as SQL and NL difficulty metrics for the data kept and data discarded from nl_evaluation.py and sql_evaluation.py
for each experiment, it returns a folder with the following:
a csv file with all evaluation metrics for the experiment, including the sql and nl difficulty metrics for the data kept and data discarded
any png files generated by visualize_metrics.py
"""

import pandas as pd
import json
import os
import sys
import numpy as np
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
from pathlib import Path

# Add project root to path (must be before imports)
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), "../../"))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

# Import evaluation functions
from evaluation.eval_metrics import (
    words, columns_used, subqueries, operators, splits, nodes,
    avg_words, avg_columns_used, avg_subqueries, avg_operators, avg_splits
)
from evaluation.visualize_metrics import create_distribution_plots


def calculate_classification_metrics(y_true, y_pred):
    """Calculate classification metrics."""
    accuracy = accuracy_score(y_true, y_pred)
    precision = precision_score(y_true, y_pred, zero_division=0)
    recall = recall_score(y_true, y_pred, zero_division=0)
    f1 = f1_score(y_true, y_pred, zero_division=0)
    
    # Calculate specificity (True Negative Rate)
    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()
    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
    
    # Confusion matrix
    cm = confusion_matrix(y_true, y_pred)
    
    return {
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1_score': f1,
        'specificity': specificity,
        'confusion_matrix': cm.tolist(),
        'true_positives': int(tp),
        'true_negatives': int(tn),
        'false_positives': int(fp),
        'false_negatives': int(fn)
    }


def calculate_sql_difficulty_metrics(df, sql_column='sql_queries'):
    """Calculate SQL difficulty metrics."""
    if sql_column not in df.columns or df[sql_column].isna().all():
        return {}
    
    try:
        return {
            'avg_words': avg_words(df, sql_column),
            'avg_columns': avg_columns_used(df, sql_column),
            'avg_subqueries': avg_subqueries(df, sql_column),
            'avg_operators': avg_operators(df, sql_column),
            'avg_splits': avg_splits(df, sql_column),
        }
    except Exception as e:
        print(f"  Warning: Error calculating SQL metrics: {e}")
        return {}


def calculate_nl_difficulty_metrics(df, nl_column='nl_questions'):
    """Calculate NL difficulty metrics using the same metrics as nl_evaluation.py."""
    if nl_column not in df.columns or df[nl_column].isna().all():
        return {}
    
    try:
        import spacy
        from spacy_syllables import SpacySyllables
        from evaluation.nl_evaluation import calculate_syntactic_complexity
        
        # Load spaCy model (only once)
        if not hasattr(calculate_nl_difficulty_metrics, 'nlp'):
            calculate_nl_difficulty_metrics.nlp = spacy.load("en_core_web_sm")
            calculate_nl_difficulty_metrics.nlp.add_pipe("syllables", after="tagger")
        
        nlp = calculate_nl_difficulty_metrics.nlp
        
        word_counts = []
        syllable_counts = []
        sentence_counts = []
        token_counts = []
        entity_counts = []
        tokens_before_verb_counts = []
        constituents_per_word_counts = []
        subordinate_clauses_counts = []
        coordinate_clauses_counts = []
        complexity_scores = []
        fk_reading_ease_scores = []
        fk_grade_level_scores = []
        
        for idx, row in df.iterrows():
            nl = str(row[nl_column])
            if pd.isna(row[nl_column]) or not nl.strip():
                continue
                
            doc = nlp(nl)
            
            # Count words (excluding punctuation)
            words = [token.text for token in doc if not token.is_punct and not token.is_space]
            word_counts.append(len(words))
            
            # Count syllables
            syllables = [token._.syllables for token in doc if not token.is_punct and not token.is_space]
            total_syllables = sum(len(s) for s in syllables if s)
            syllable_counts.append(total_syllables)
            
            # Count sentences
            sentence_counts.append(len(list(doc.sents)))
            
            # Count tokens
            token_counts.append(len(doc))
            
            # Count entities
            entity_counts.append(len(doc.ents))
            
            # Calculate syntactic complexity (using nl_evaluation.py function)
            syntactic_metrics = calculate_syntactic_complexity(doc)
            tokens_before_verb_counts.append(syntactic_metrics['tokens_before_main_verb'])
            constituents_per_word_counts.append(syntactic_metrics['constituents_per_word'])
            subordinate_clauses_counts.append(syntactic_metrics['subordinate_clauses'])
            coordinate_clauses_counts.append(syntactic_metrics['coordinate_clauses'])
            complexity_scores.append(syntactic_metrics['complexity_score'])
            
            # Calculate Flesch-Kincaid scores (same formula as nl_evaluation.py)
            if len(sentence_counts) > 0 and word_counts[-1] > 0:
                avg_sentence_length = word_counts[-1] / sentence_counts[-1] if sentence_counts[-1] > 0 else 0
                avg_syllables_per_word = total_syllables / word_counts[-1] if word_counts[-1] > 0 else 0
                
                # Flesch-Kincaid Reading Ease
                fk_reading_ease = 206.835 - (1.015 * avg_sentence_length) - (84.6 * avg_syllables_per_word)
                fk_reading_ease_scores.append(fk_reading_ease)
                
                # Flesch-Kincaid Grade Level
                fk_grade_level = 0.39 * avg_sentence_length + 11.8 * avg_syllables_per_word - 15.59
                fk_grade_level_scores.append(fk_grade_level)
            else:
                fk_reading_ease_scores.append(0)
                fk_grade_level_scores.append(0)
        
        if not word_counts:
            return {}
        
        return {
            'avg_words': np.mean(word_counts) if word_counts else 0,
            'avg_syllables': np.mean(syllable_counts) if syllable_counts else 0,
            'avg_sentences': np.mean(sentence_counts) if sentence_counts else 0,
            'avg_tokens': np.mean(token_counts) if token_counts else 0,
            'avg_entities': np.mean(entity_counts) if entity_counts else 0,
            'avg_tokens_before_verb': np.mean(tokens_before_verb_counts) if tokens_before_verb_counts else 0,
            'avg_constituents_per_word': np.mean(constituents_per_word_counts) if constituents_per_word_counts else 0,
            'avg_subordinate_clauses': np.mean(subordinate_clauses_counts) if subordinate_clauses_counts else 0,
            'avg_coordinate_clauses': np.mean(coordinate_clauses_counts) if coordinate_clauses_counts else 0,
            'avg_complexity_score': np.mean(complexity_scores) if complexity_scores else 0,
            'avg_flesch_reading_ease': np.mean(fk_reading_ease_scores) if fk_reading_ease_scores else 0,
            'avg_flesch_kincaid_grade_level': np.mean(fk_grade_level_scores) if fk_grade_level_scores else 0,
        }
    except Exception as e:
        print(f"  Warning: Error calculating NL metrics: {e}")
        import traceback
        traceback.print_exc()
        return {}


def evaluate_experiment(experiment_name, filtered_scores_path, ground_truth_column='allignment', 
                       score_column=None, output_dir=None, reward_column=None, reward_threshold=0.6,
                       score_threshold_column=None, score_threshold_value=None):
    """Evaluate a single filtering experiment.
    
    Args:
        experiment_name: Name of the experiment
        filtered_scores_path: Path to CSV with filtered scores
        ground_truth_column: Column name for ground truth labels
        score_column: Column name for filtered scores (binary 0/1)
        output_dir: Directory to save evaluation results
        reward_column: If provided, use this column with threshold instead of score_column
        reward_threshold: Threshold for reward_column (accept if > threshold)
        score_threshold_column: If provided, use this column (e.g., prompt_scores) with threshold
        score_threshold_value: Threshold for score_threshold_column (accept if >= threshold)
    """
    print(f"\n{'='*60}")
    print(f"Evaluating: {experiment_name}")
    print(f"{'='*60}")
    
    # Load the filtered scores file
    if not os.path.exists(filtered_scores_path):
        print(f"âŒ File not found: {filtered_scores_path}")
        return None
    
    df = pd.read_csv(filtered_scores_path)
    print(f"âœ… Loaded {len(df)} examples")
    
    # Handle reward-based filtering
    if reward_column is not None:
        if reward_column not in df.columns:
            print(f"âŒ Could not find reward column '{reward_column}'")
            print(f"   Available columns: {list(df.columns)}")
            return None
        
        # Create filtered scores based on reward threshold
        print(f"ðŸ“Š Using reward column '{reward_column}' with threshold {reward_threshold}")
        df['_computed_filtered_scores'] = (df[reward_column].fillna(0).astype(float) > reward_threshold).astype(int)
        score_column = '_computed_filtered_scores'
    # Handle score-based filtering (e.g., prompt_scores)
    elif score_threshold_column is not None:
        if score_threshold_column not in df.columns:
            print(f"âŒ Could not find score column '{score_threshold_column}'")
            print(f"   Available columns: {list(df.columns)}")
            return None
        
        # Create filtered scores based on score threshold
        print(f"ðŸ“Š Using score column '{score_threshold_column}' with threshold {score_threshold_value}")
        df['_computed_filtered_scores'] = (df[score_threshold_column].fillna(0).astype(float) >= score_threshold_value).astype(int)
        score_column = '_computed_filtered_scores'
    else:
        # Find the filtered scores column
        if score_column is None:
            # Try to find common column names
            possible_columns = [
                f"{experiment_name}_filtered_scores",
                "filtered_scores",
                "finetuned_filtered_scores",
                "llama_filtered_scores",
                "prompt_filtered_scores",
                "reward_filtered_scores",
            ]
            score_column = None
            for col in possible_columns:
                if col in df.columns:
                    score_column = col
                    break
            
            if score_column is None:
                # Find any column ending with _filtered_scores
                for col in df.columns:
                    if col.endswith('_filtered_scores'):
                        score_column = col
                        break
        
        if score_column is None or score_column not in df.columns:
            print(f"âŒ Could not find filtered scores column in {filtered_scores_path}")
            print(f"   Available columns: {list(df.columns)}")
            return None
    
    if ground_truth_column not in df.columns:
        print(f"âŒ Could not find ground truth column '{ground_truth_column}'")
        return None
    
    # Get predictions and ground truth
    y_pred = df[score_column].fillna(0).astype(int)
    y_true = df[ground_truth_column].fillna(0).astype(int)
    
    # Calculate classification metrics
    print("ðŸ“Š Calculating classification metrics...")
    metrics = calculate_classification_metrics(y_true, y_pred)
    
    # Split data into kept (accepted) and discarded (rejected)
    kept_df = df[y_pred == 1].copy()
    discarded_df = df[y_pred == 0].copy()
    
    print(f"   Kept: {len(kept_df)} examples")
    print(f"   Discarded: {len(discarded_df)} examples")
    
    # Calculate SQL difficulty metrics for kept and discarded
    print("ðŸ“Š Calculating SQL difficulty metrics...")
    sql_kept = calculate_sql_difficulty_metrics(kept_df, 'sql_queries')
    sql_discarded = calculate_sql_difficulty_metrics(discarded_df, 'sql_queries')
    
    # Calculate NL difficulty metrics for kept and discarded
    print("ðŸ“Š Calculating NL difficulty metrics...")
    nl_kept = calculate_nl_difficulty_metrics(kept_df, 'nl_questions')
    nl_discarded = calculate_nl_difficulty_metrics(discarded_df, 'nl_questions')
    
    # Combine all metrics
    all_metrics = {
        'experiment_name': experiment_name,
        **metrics,
        'sql_kept': sql_kept,
        'sql_discarded': sql_discarded,
        'nl_kept': nl_kept,
        'nl_discarded': nl_discarded,
        'num_kept': len(kept_df),
        'num_discarded': len(discarded_df),
        'total_examples': len(df)
    }
    
    # Create output directory
    if output_dir is None:
        output_dir = os.path.join(project_root, f"filtering_experiments/{experiment_name}_evaluation")
    os.makedirs(output_dir, exist_ok=True)
    
    # Save metrics to CSV (flatten nested dicts)
    metrics_df = pd.DataFrame([flatten_metrics(all_metrics)])
    metrics_path = os.path.join(output_dir, "evaluation_metrics.csv")
    metrics_df.to_csv(metrics_path, index=False)
    print(f"âœ… Metrics saved to {metrics_path}")
    
    # Generate visualizations for kept and discarded data
    print("ðŸ“Š Generating visualizations...")
    try:
        import matplotlib
        matplotlib.use('Agg')  # Use non-interactive backend
        import matplotlib.pyplot as plt
        
        if len(kept_df) > 0:
            kept_path = os.path.join(output_dir, "kept_data.csv")
            kept_df.to_csv(kept_path, index=False)
            create_distribution_plots(kept_path, 'sql_queries', f'{experiment_name}_kept')
            # Move generated plot to output directory
            plot_name = f'Distribution_of_SQL_Query_Complexity_Metrics_in_the_{experiment_name}_kept_dataset.png'
            if os.path.exists(plot_name):
                os.rename(plot_name, os.path.join(output_dir, plot_name))
        
        if len(discarded_df) > 0:
            discarded_path = os.path.join(output_dir, "discarded_data.csv")
            discarded_df.to_csv(discarded_path, index=False)
            create_distribution_plots(discarded_path, 'sql_queries', f'{experiment_name}_discarded')
            # Move generated plot to output directory
            plot_name = f'Distribution_of_SQL_Query_Complexity_Metrics_in_the_{experiment_name}_discarded_dataset.png'
            if os.path.exists(plot_name):
                os.rename(plot_name, os.path.join(output_dir, plot_name))
    except Exception as e:
        print(f"  Warning: Could not generate visualizations: {e}")
        import traceback
        traceback.print_exc()
    
    return all_metrics


def flatten_metrics(metrics_dict, prefix=''):
    """Flatten nested dictionary for CSV export."""
    flattened = {}
    for key, value in metrics_dict.items():
        new_key = f"{prefix}_{key}" if prefix else key
        if isinstance(value, dict):
            flattened.update(flatten_metrics(value, new_key))
        elif isinstance(value, list):
            # Handle confusion matrix
            if key == 'confusion_matrix':
                flattened[f'{new_key}_00'] = value[0][0] if len(value) > 0 and len(value[0]) > 0 else 0
                flattened[f'{new_key}_01'] = value[0][1] if len(value) > 0 and len(value[0]) > 1 else 0
                flattened[f'{new_key}_10'] = value[1][0] if len(value) > 1 and len(value[1]) > 0 else 0
                flattened[f'{new_key}_11'] = value[1][1] if len(value) > 1 and len(value[1]) > 1 else 0
            else:
                flattened[new_key] = str(value)
        else:
            flattened[new_key] = value
    return flattened


def evaluate_filtering_experiments():
    """
    Evaluate all filtering experiments
    """
    # Define experiments to evaluate
    experiments = [
        {
            'name': 'finetuned_qwen',
            'path': os.path.join(project_root, 'filtering_experiments/source_2_synth_rep/finetuned_filtered_scores.csv'),
            'score_column': 'finetuned_filtered_scores'
        },
        {
            'name': 'llama_teacher',
            'path': os.path.join(project_root, 'filtering_experiments/teacher_model_filter.py/llama_filtered_scores.csv'),
            'score_column': 'llama_filtered_scores'
        },
        {
            'name': 'prompt_filter_threshold_0.75',
            'path': os.path.join(project_root, 'filtering_experiments/prompt_filter.py/prompt_filtered_scores.csv'),
            'score_threshold_column': 'prompt_scores',
            'score_threshold_value': 0.75
        },
        {
            'name': 'prompt_filter_threshold_0.8',
            'path': os.path.join(project_root, 'filtering_experiments/prompt_filter.py/prompt_filtered_scores.csv'),
            'score_threshold_column': 'prompt_scores',
            'score_threshold_value': 0.8
        },
        {
            'name': 'prompt_filter_threshold_0.85',
            'path': os.path.join(project_root, 'filtering_experiments/prompt_filter.py/prompt_filtered_scores.csv'),
            'score_threshold_column': 'prompt_scores',
            'score_threshold_value': 0.85
        },
        {
            'name': 'prompt_filter_threshold_0.9',
            'path': os.path.join(project_root, 'filtering_experiments/prompt_filter.py/prompt_filtered_scores.csv'),
            'score_threshold_column': 'prompt_scores',
            'score_threshold_value': 0.9
        },
        {
            'name': 'reward_threshold_0.55',
            'path': os.path.join(project_root, 'filtering_experiments/source_2_synth_rep/finetuned_filtered_scores.csv'),
            'reward_column': 'rewards',
            'reward_threshold': 0.55
        },
        {
            'name': 'reward_threshold_0.6',
            'path': os.path.join(project_root, 'filtering_experiments/source_2_synth_rep/finetuned_filtered_scores.csv'),
            'reward_column': 'rewards',
            'reward_threshold': 0.6
        },
        {
            'name': 'reward_threshold_0.65',
            'path': os.path.join(project_root, 'filtering_experiments/source_2_synth_rep/finetuned_filtered_scores.csv'),
            'reward_column': 'rewards',
            'reward_threshold': 0.65
        },
        {
            'name': 'reward_threshold_0.7',
            'path': os.path.join(project_root, 'filtering_experiments/source_2_synth_rep/finetuned_filtered_scores.csv'),
            'reward_column': 'rewards',
            'reward_threshold': 0.7
        },
    ]
    
    all_results = []
    
    for exp in experiments:
        if os.path.exists(exp['path']):
            result = evaluate_experiment(
                exp['name'],
                exp['path'],
                ground_truth_column='allignment',
                score_column=exp.get('score_column'),
                reward_column=exp.get('reward_column'),
                reward_threshold=exp.get('reward_threshold', 0.6),
                score_threshold_column=exp.get('score_threshold_column'),
                score_threshold_value=exp.get('score_threshold_value')
            )
            if result:
                all_results.append(result)
        else:
            print(f"âš ï¸  Skipping {exp['name']}: file not found at {exp['path']}")
    
    # Create comprehensive comparison CSV with all metrics
    if all_results:
        print(f"\n{'='*60}")
        print("Creating comprehensive metrics comparison...")
        print(f"{'='*60}")
        
        # Build comprehensive metrics list
        comprehensive_data = []
        for result in all_results:
            row = {
                'experiment': result['experiment_name'],
                # Classification metrics
                'accuracy': result['accuracy'],
                'precision': result['precision'],
                'recall': result['recall'],
                'f1_score': result['f1_score'],
                'specificity': result['specificity'],
                'true_positives': result['true_positives'],
                'true_negatives': result['true_negatives'],
                'false_positives': result['false_positives'],
                'false_negatives': result['false_negatives'],
                # Counts
                'num_kept': result['num_kept'],
                'num_discarded': result['num_discarded'],
                'total_examples': result['total_examples'],
                'keep_rate': result['num_kept'] / result['total_examples'] if result['total_examples'] > 0 else 0,
            }
            
            # SQL metrics for kept data
            sql_kept = result.get('sql_kept', {})
            row.update({
                'sql_kept_avg_words': sql_kept.get('avg_words', 0),
                'sql_kept_avg_columns': sql_kept.get('avg_columns', 0),
                'sql_kept_avg_subqueries': sql_kept.get('avg_subqueries', 0),
                'sql_kept_avg_operators': sql_kept.get('avg_operators', 0),
                'sql_kept_avg_splits': sql_kept.get('avg_splits', 0),
            })
            
            # SQL metrics for discarded data
            sql_discarded = result.get('sql_discarded', {})
            row.update({
                'sql_discarded_avg_words': sql_discarded.get('avg_words', 0),
                'sql_discarded_avg_columns': sql_discarded.get('avg_columns', 0),
                'sql_discarded_avg_subqueries': sql_discarded.get('avg_subqueries', 0),
                'sql_discarded_avg_operators': sql_discarded.get('avg_operators', 0),
                'sql_discarded_avg_splits': sql_discarded.get('avg_splits', 0),
            })
            
            # NL metrics for kept data (using metrics from nl_evaluation.py)
            nl_kept = result.get('nl_kept', {})
            row.update({
                'nl_kept_avg_words': nl_kept.get('avg_words', 0),
                'nl_kept_avg_syllables': nl_kept.get('avg_syllables', 0),
                'nl_kept_avg_sentences': nl_kept.get('avg_sentences', 0),
                'nl_kept_avg_tokens': nl_kept.get('avg_tokens', 0),
                'nl_kept_avg_entities': nl_kept.get('avg_entities', 0),
                'nl_kept_avg_tokens_before_verb': nl_kept.get('avg_tokens_before_verb', 0),
                'nl_kept_avg_constituents_per_word': nl_kept.get('avg_constituents_per_word', 0),
                'nl_kept_avg_subordinate_clauses': nl_kept.get('avg_subordinate_clauses', 0),
                'nl_kept_avg_coordinate_clauses': nl_kept.get('avg_coordinate_clauses', 0),
                'nl_kept_avg_complexity_score': nl_kept.get('avg_complexity_score', 0),
                'nl_kept_avg_flesch_reading_ease': nl_kept.get('avg_flesch_reading_ease', 0),
                'nl_kept_avg_flesch_kincaid_grade_level': nl_kept.get('avg_flesch_kincaid_grade_level', 0),
            })
            
            # NL metrics for discarded data (using metrics from nl_evaluation.py)
            nl_discarded = result.get('nl_discarded', {})
            row.update({
                'nl_discarded_avg_words': nl_discarded.get('avg_words', 0),
                'nl_discarded_avg_syllables': nl_discarded.get('avg_syllables', 0),
                'nl_discarded_avg_sentences': nl_discarded.get('avg_sentences', 0),
                'nl_discarded_avg_tokens': nl_discarded.get('avg_tokens', 0),
                'nl_discarded_avg_entities': nl_discarded.get('avg_entities', 0),
                'nl_discarded_avg_tokens_before_verb': nl_discarded.get('avg_tokens_before_verb', 0),
                'nl_discarded_avg_constituents_per_word': nl_discarded.get('avg_constituents_per_word', 0),
                'nl_discarded_avg_subordinate_clauses': nl_discarded.get('avg_subordinate_clauses', 0),
                'nl_discarded_avg_coordinate_clauses': nl_discarded.get('avg_coordinate_clauses', 0),
                'nl_discarded_avg_complexity_score': nl_discarded.get('avg_complexity_score', 0),
                'nl_discarded_avg_flesch_reading_ease': nl_discarded.get('avg_flesch_reading_ease', 0),
                'nl_discarded_avg_flesch_kincaid_grade_level': nl_discarded.get('avg_flesch_kincaid_grade_level', 0),
            })
            
            comprehensive_data.append(row)
        
        # Create DataFrame and save
        comprehensive_df = pd.DataFrame(comprehensive_data)
        comprehensive_path = os.path.join(project_root, 'filtering_experiments/useful_eval_results_and_scripts/all_experiments_metrics.csv')
        comprehensive_df.to_csv(comprehensive_path, index=False)
        print(f"âœ… Comprehensive metrics saved to {comprehensive_path}")
        
        # Also create a simpler summary for quick viewing
        summary_data = []
        for result in all_results:
            summary_data.append({
                'experiment': result['experiment_name'],
                'accuracy': result['accuracy'],
                'precision': result['precision'],
                'recall': result['recall'],
                'f1_score': result['f1_score'],
                'specificity': result['specificity'],
                'num_kept': result['num_kept'],
                'num_discarded': result['num_discarded'],
                'keep_rate': result['num_kept'] / result['total_examples'] if result['total_examples'] > 0 else 0,
            })
        
        summary_df = pd.DataFrame(summary_data)
        summary_path = os.path.join(project_root, 'filtering_experiments/useful_eval_results_and_scripts/comparison_summary.csv')
        summary_df.to_csv(summary_path, index=False)
        print(f"âœ… Summary saved to {summary_path}")
        print("\n" + summary_df.to_string(index=False))
    
    print(f"\nâœ… Evaluation complete!")


if __name__ == "__main__":
    evaluate_filtering_experiments()
